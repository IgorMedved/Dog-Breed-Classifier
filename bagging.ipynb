{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   9.00000000e-01,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04,   7.51879699e-04,   7.51879699e-04,\n",
       "         7.51879699e-04])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for target in train_targets:\n",
    "    for n,value in enumerate(target):\n",
    "        if value < .1:\n",
    "            target[n] = .1/len(target)\n",
    "        else:\n",
    "            target[n] = .9\n",
    "train_targets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6680, 7, 7, 2048)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_xception.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "bottleneck_features_x = np.load('bottleneck_features/DogXceptionData.npz')\n",
    "train_xception = bottleneck_features_x['train']\n",
    "valid_xception = bottleneck_features_x['valid']\n",
    "test_xception = bottleneck_features_x['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 7, 7, 512)         1049088   \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 7, 7, 2048)        9439232   \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 7, 7, 2048)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_72  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_141 (Bat (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "batch_normalization_142 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 133)               34181     \n",
      "=================================================================\n",
      "Total params: 11,056,261\n",
      "Trainable params: 11,051,653\n",
      "Non-trainable params: 4,608\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, BatchNormalization, Activation\n",
    "from keras.models import Sequential\n",
    "xception_model = Sequential()\n",
    "xception_model.add(Conv2D(filters=512, kernel_size=1, padding='same', activation='relu', input_shape=(7,7,2048)))\n",
    "xception_model.add(Conv2D(filters = 2048, kernel_size =3,padding='same', activation = 'relu'))\n",
    "xception_model.add(Dropout(.3))\n",
    "xception_model.add(GlobalAveragePooling2D())\n",
    "xception_model.add(BatchNormalization())\n",
    "xception_model.add(Dense(256, activation='relu'))\n",
    "xception_model.add(BatchNormalization())\n",
    "xception_model.add(Dropout(.3))\n",
    "#\n",
    "xception_model.add(Dense(133, activation='softmax'))\n",
    "#xception_model.add(Activation('softmax'))\n",
    "\n",
    "xception_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xception_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6600/6680 [============================>.] - ETA: 8s - loss: 2.0919 - acc: 0.6611 Epoch 00001: val_loss improved from inf to 1.86072, saving model to saved_models/weights.bagging2.hdf5\n",
      "6680/6680 [==============================] - 692s 104ms/step - loss: 2.0860 - acc: 0.6626 - val_loss: 1.8607 - val_acc: 0.5593\n",
      "Epoch 2/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 1.4422 - acc: 0.8277 Epoch 00002: val_loss improved from 1.86072 to 0.94612, saving model to saved_models/weights.bagging2.hdf5\n",
      "6680/6680 [==============================] - 658s 99ms/step - loss: 1.4448 - acc: 0.8262 - val_loss: 0.9461 - val_acc: 0.7509\n",
      "Epoch 3/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 1.3179 - acc: 0.8711 Epoch 00003: val_loss improved from 0.94612 to 0.71698, saving model to saved_models/weights.bagging2.hdf5\n",
      "6680/6680 [==============================] - 635s 95ms/step - loss: 1.3186 - acc: 0.8711 - val_loss: 0.7170 - val_acc: 0.8036\n",
      "Epoch 4/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 1.2288 - acc: 0.9065 Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 631s 94ms/step - loss: 1.2289 - acc: 0.9066 - val_loss: 0.7576 - val_acc: 0.8060\n",
      "Epoch 5/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 1.1486 - acc: 0.9342 Epoch 00005: val_loss improved from 0.71698 to 0.70115, saving model to saved_models/weights.bagging2.hdf5\n",
      "6680/6680 [==============================] - 627s 94ms/step - loss: 1.1487 - acc: 0.9341 - val_loss: 0.7011 - val_acc: 0.8287\n",
      "Epoch 6/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 1.0950 - acc: 0.9530 Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 628s 94ms/step - loss: 1.0958 - acc: 0.9530 - val_loss: 0.7267 - val_acc: 0.8180\n",
      "Epoch 7/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 1.0576 - acc: 0.9668 Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 623s 93ms/step - loss: 1.0579 - acc: 0.9663 - val_loss: 0.7200 - val_acc: 0.8204\n",
      "Epoch 8/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 1.0294 - acc: 0.9738 Epoch 00008: val_loss improved from 0.70115 to 0.65456, saving model to saved_models/weights.bagging2.hdf5\n",
      "6680/6680 [==============================] - 626s 94ms/step - loss: 1.0287 - acc: 0.9741 - val_loss: 0.6546 - val_acc: 0.8407\n",
      "Epoch 9/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 1.0049 - acc: 0.9785 Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 621s 93ms/step - loss: 1.0051 - acc: 0.9786 - val_loss: 0.6892 - val_acc: 0.8228\n",
      "Epoch 10/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 0.9884 - acc: 0.9812 Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 624s 93ms/step - loss: 0.9881 - acc: 0.9814 - val_loss: 0.7075 - val_acc: 0.8275\n",
      "Epoch 11/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 0.9632 - acc: 0.9891 Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 624s 93ms/step - loss: 0.9635 - acc: 0.9888 - val_loss: 0.6936 - val_acc: 0.8335\n",
      "Epoch 12/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 0.9557 - acc: 0.9889 Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 626s 94ms/step - loss: 0.9560 - acc: 0.9888 - val_loss: 0.7374 - val_acc: 0.8335\n",
      "Epoch 13/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 0.9400 - acc: 0.9935 Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 628s 94ms/step - loss: 0.9400 - acc: 0.9933 - val_loss: 0.7022 - val_acc: 0.8251\n",
      "Epoch 14/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 0.9273 - acc: 0.9941 Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 623s 93ms/step - loss: 0.9280 - acc: 0.9937 - val_loss: 0.6931 - val_acc: 0.8527\n",
      "Epoch 15/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 0.9276 - acc: 0.9935 Epoch 00015: val_loss improved from 0.65456 to 0.63730, saving model to saved_models/weights.bagging2.hdf5\n",
      "6680/6680 [==============================] - 625s 94ms/step - loss: 0.9277 - acc: 0.9936 - val_loss: 0.6373 - val_acc: 0.8395\n",
      "Epoch 16/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 0.9161 - acc: 0.9961 Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 622s 93ms/step - loss: 0.9162 - acc: 0.9961 - val_loss: 0.6731 - val_acc: 0.8491\n",
      "Epoch 17/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 0.9124 - acc: 0.9950 Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 622s 93ms/step - loss: 0.9122 - acc: 0.9951 - val_loss: 0.6896 - val_acc: 0.8455\n",
      "Epoch 18/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 0.9028 - acc: 0.9967 Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 627s 94ms/step - loss: 0.9026 - acc: 0.9967 - val_loss: 0.7133 - val_acc: 0.8383\n",
      "Epoch 19/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 0.8984 - acc: 0.9964 Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 625s 94ms/step - loss: 0.8984 - acc: 0.9964 - val_loss: 0.6755 - val_acc: 0.8539\n",
      "Epoch 20/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 0.8949 - acc: 0.9970 Epoch 00020: val_loss did not improve\n",
      "6680/6680 [==============================] - 625s 94ms/step - loss: 0.8948 - acc: 0.9969 - val_loss: 0.7159 - val_acc: 0.8443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18e67526908>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.bagging2.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "xception_model.fit(train_xception, train_targets, \n",
    "          validation_data=(valid_xception, valid_targets),\n",
    "          epochs=20, batch_size=100, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 84.5694%\n"
     ]
    }
   ],
   "source": [
    "xception_predictions = [np.argmax(xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_xception]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(xception_predictions)==np.argmax(test_targets, axis=1))/len(xception_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xception_model.save_weights('saved_models/xceptbag.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xception_model.load_weights('saved_models/weights.bagging.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 7s 1ms/step - loss: 2.0934 - acc: 0.6771 - val_loss: 1.9025 - val_acc: 0.8096\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 4s 655us/step - loss: 1.2493 - acc: 0.9027 - val_loss: 1.5058 - val_acc: 0.8407\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 4s 641us/step - loss: 1.1318 - acc: 0.9484 - val_loss: 1.1952 - val_acc: 0.8347\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 4s 641us/step - loss: 1.0739 - acc: 0.9665 - val_loss: 0.9909 - val_acc: 0.8359\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 4s 648us/step - loss: 1.0336 - acc: 0.9790 - val_loss: 0.8453 - val_acc: 0.8204\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 4s 657us/step - loss: 1.0071 - acc: 0.9855 - val_loss: 0.7548 - val_acc: 0.8491\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 4s 667us/step - loss: 0.9907 - acc: 0.9891 - val_loss: 0.7255 - val_acc: 0.8515\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 4s 657us/step - loss: 0.9755 - acc: 0.9919 - val_loss: 0.6972 - val_acc: 0.8479\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 4s 655us/step - loss: 0.9648 - acc: 0.9957 - val_loss: 0.6900 - val_acc: 0.8395\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 4s 657us/step - loss: 0.9539 - acc: 0.9957 - val_loss: 0.7042 - val_acc: 0.8443\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 7s 1ms/step - loss: 2.0942 - acc: 0.6768 - val_loss: 1.8860 - val_acc: 0.8156\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 4s 648us/step - loss: 1.2529 - acc: 0.9013 - val_loss: 1.4732 - val_acc: 0.8383\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 4s 639us/step - loss: 1.1336 - acc: 0.9455 - val_loss: 1.1678 - val_acc: 0.8263\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 4s 641us/step - loss: 1.0698 - acc: 0.9654 - val_loss: 0.9597 - val_acc: 0.8371\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 4s 639us/step - loss: 1.0349 - acc: 0.9766 - val_loss: 0.8184 - val_acc: 0.8491\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 4s 655us/step - loss: 1.0094 - acc: 0.9832 - val_loss: 0.7761 - val_acc: 0.8383\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 4s 641us/step - loss: 0.9915 - acc: 0.9895 - val_loss: 0.7134 - val_acc: 0.8419\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 4s 639us/step - loss: 0.9759 - acc: 0.9918 - val_loss: 0.7108 - val_acc: 0.8419\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 4s 641us/step - loss: 0.9659 - acc: 0.9955 - val_loss: 0.7108 - val_acc: 0.8515\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 4s 660us/step - loss: 0.9537 - acc: 0.9964 - val_loss: 0.6972 - val_acc: 0.8455\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 7s 1ms/step - loss: 2.1098 - acc: 0.6717 - val_loss: 1.9233 - val_acc: 0.8060\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 4s 641us/step - loss: 1.2370 - acc: 0.9066 - val_loss: 1.5044 - val_acc: 0.8251\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 4s 639us/step - loss: 1.1356 - acc: 0.9437 - val_loss: 1.2112 - val_acc: 0.8204\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 4s 646us/step - loss: 1.0744 - acc: 0.9669 - val_loss: 0.9740 - val_acc: 0.8335\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 4s 650us/step - loss: 1.0330 - acc: 0.9787 - val_loss: 0.8514 - val_acc: 0.8371\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 4s 648us/step - loss: 1.0043 - acc: 0.9864 - val_loss: 0.7676 - val_acc: 0.8443\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 4s 653us/step - loss: 0.9943 - acc: 0.9868 - val_loss: 0.7464 - val_acc: 0.8299\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 4s 662us/step - loss: 0.9793 - acc: 0.9918 - val_loss: 0.7210 - val_acc: 0.8431\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 4s 655us/step - loss: 0.9644 - acc: 0.9945 - val_loss: 0.7021 - val_acc: 0.8431\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 4s 648us/step - loss: 0.9555 - acc: 0.9948 - val_loss: 0.7318 - val_acc: 0.8371\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 7s 1ms/step - loss: 2.0978 - acc: 0.6819 - val_loss: 1.9001 - val_acc: 0.8084\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 4s 657us/step - loss: 1.2577 - acc: 0.9012 - val_loss: 1.4970 - val_acc: 0.8132\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 4s 639us/step - loss: 1.1311 - acc: 0.9425 - val_loss: 1.1814 - val_acc: 0.8335\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 4s 646us/step - loss: 1.0713 - acc: 0.9659 - val_loss: 0.9647 - val_acc: 0.8359\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 4s 636us/step - loss: 1.0352 - acc: 0.9789 - val_loss: 0.8262 - val_acc: 0.8419\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 5s 681us/step - loss: 1.0124 - acc: 0.9837 - val_loss: 0.7611 - val_acc: 0.8455\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 5s 721us/step - loss: 0.9895 - acc: 0.9900 - val_loss: 0.7257 - val_acc: 0.8419\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 5s 737us/step - loss: 0.9764 - acc: 0.9906 - val_loss: 0.7092 - val_acc: 0.8443\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 5s 690us/step - loss: 0.9634 - acc: 0.9943 - val_loss: 0.7031 - val_acc: 0.8515\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 5s 709us/step - loss: 0.9517 - acc: 0.9966 - val_loss: 0.6869 - val_acc: 0.8527\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 8s 1ms/step - loss: 2.1193 - acc: 0.6766 - val_loss: 1.9310 - val_acc: 0.8000\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 5s 697us/step - loss: 1.2584 - acc: 0.9024 - val_loss: 1.5184 - val_acc: 0.8299\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 5s 697us/step - loss: 1.1351 - acc: 0.9446 - val_loss: 1.1906 - val_acc: 0.8455\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 5s 692us/step - loss: 1.0747 - acc: 0.9677 - val_loss: 0.9600 - val_acc: 0.8539\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 5s 697us/step - loss: 1.0365 - acc: 0.9784 - val_loss: 0.8328 - val_acc: 0.8383\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 5s 699us/step - loss: 1.0068 - acc: 0.9846 - val_loss: 0.7656 - val_acc: 0.8395\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 4s 664us/step - loss: 0.9918 - acc: 0.9880 - val_loss: 0.7371 - val_acc: 0.8467\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 4s 662us/step - loss: 0.9746 - acc: 0.9934 - val_loss: 0.7193 - val_acc: 0.8371\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 5s 746us/step - loss: 0.9660 - acc: 0.9939 - val_loss: 0.6884 - val_acc: 0.8431\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 5s 723us/step - loss: 0.9564 - acc: 0.9955 - val_loss: 0.6845 - val_acc: 0.8431\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 8s 1ms/step - loss: 2.1025 - acc: 0.6799 - val_loss: 1.8860 - val_acc: 0.8132\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 4s 667us/step - loss: 1.2528 - acc: 0.9045 - val_loss: 1.4935 - val_acc: 0.8299\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 4s 669us/step - loss: 1.1356 - acc: 0.9445 - val_loss: 1.2044 - val_acc: 0.8299\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 5s 678us/step - loss: 1.0740 - acc: 0.9651 - val_loss: 0.9787 - val_acc: 0.8395\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 5s 739us/step - loss: 1.0393 - acc: 0.9756 - val_loss: 0.8087 - val_acc: 0.8491\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 5s 676us/step - loss: 1.0104 - acc: 0.9826 - val_loss: 0.7492 - val_acc: 0.8515\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 5s 704us/step - loss: 0.9885 - acc: 0.9897 - val_loss: 0.7486 - val_acc: 0.8419\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 4s 646us/step - loss: 0.9754 - acc: 0.9915 - val_loss: 0.7179 - val_acc: 0.8383\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 4s 636us/step - loss: 0.9638 - acc: 0.9946 - val_loss: 0.7093 - val_acc: 0.8491\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 4s 643us/step - loss: 0.9567 - acc: 0.9951 - val_loss: 0.7019 - val_acc: 0.8455\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 7s 1ms/step - loss: 2.0987 - acc: 0.6834 - val_loss: 1.9033 - val_acc: 0.8060\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 4s 636us/step - loss: 1.2535 - acc: 0.8994 - val_loss: 1.4887 - val_acc: 0.8335\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 4s 643us/step - loss: 1.1341 - acc: 0.9436 - val_loss: 1.1769 - val_acc: 0.8479\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 4s 639us/step - loss: 1.0734 - acc: 0.9660 - val_loss: 0.9543 - val_acc: 0.8407\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 4s 636us/step - loss: 1.0350 - acc: 0.9799 - val_loss: 0.8142 - val_acc: 0.8407\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 4s 639us/step - loss: 1.0081 - acc: 0.9856 - val_loss: 0.7700 - val_acc: 0.8407\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 4s 639us/step - loss: 0.9916 - acc: 0.9889 - val_loss: 0.7441 - val_acc: 0.8431\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 4s 643us/step - loss: 0.9780 - acc: 0.9919 - val_loss: 0.7189 - val_acc: 0.8347\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 4s 641us/step - loss: 0.9669 - acc: 0.9925 - val_loss: 0.6988 - val_acc: 0.8431\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 4s 639us/step - loss: 0.9583 - acc: 0.9949 - val_loss: 0.6945 - val_acc: 0.8479\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 7s 1ms/step - loss: 2.0793 - acc: 0.6774 - val_loss: 1.9090 - val_acc: 0.8024\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 4s 653us/step - loss: 1.2431 - acc: 0.9042 - val_loss: 1.4950 - val_acc: 0.8395\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 4s 643us/step - loss: 1.1349 - acc: 0.9425 - val_loss: 1.1897 - val_acc: 0.8407\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 4s 643us/step - loss: 1.0757 - acc: 0.9608 - val_loss: 0.9855 - val_acc: 0.8419\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 5s 676us/step - loss: 1.0372 - acc: 0.9775 - val_loss: 0.8314 - val_acc: 0.8419\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 4s 653us/step - loss: 1.0154 - acc: 0.9834 - val_loss: 0.7492 - val_acc: 0.8479\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 4s 660us/step - loss: 0.9892 - acc: 0.9900 - val_loss: 0.7477 - val_acc: 0.8467\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 5s 690us/step - loss: 0.9732 - acc: 0.9925 - val_loss: 0.7237 - val_acc: 0.8395\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 5s 685us/step - loss: 0.9637 - acc: 0.9955 - val_loss: 0.7147 - val_acc: 0.8419\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 4s 655us/step - loss: 0.9554 - acc: 0.9957 - val_loss: 0.7023 - val_acc: 0.8479\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 8s 1ms/step - loss: 2.1163 - acc: 0.6714 - val_loss: 1.9477 - val_acc: 0.8048\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 4s 671us/step - loss: 1.2493 - acc: 0.8975 - val_loss: 1.5077 - val_acc: 0.8240\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 5s 676us/step - loss: 1.1301 - acc: 0.9449 - val_loss: 1.2006 - val_acc: 0.8383\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 4s 671us/step - loss: 1.0760 - acc: 0.9659 - val_loss: 0.9540 - val_acc: 0.8491\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 5s 697us/step - loss: 1.0343 - acc: 0.9792 - val_loss: 0.8153 - val_acc: 0.8539\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 5s 706us/step - loss: 1.0092 - acc: 0.9834 - val_loss: 0.7750 - val_acc: 0.8395\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 5s 735us/step - loss: 0.9898 - acc: 0.9895 - val_loss: 0.7298 - val_acc: 0.8383\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 5s 728us/step - loss: 0.9794 - acc: 0.9909 - val_loss: 0.7171 - val_acc: 0.8467\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 5s 685us/step - loss: 0.9612 - acc: 0.9942 - val_loss: 0.7198 - val_acc: 0.8503\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 4s 655us/step - loss: 0.9534 - acc: 0.9966 - val_loss: 0.7049 - val_acc: 0.8515\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 8s 1ms/step - loss: 2.1011 - acc: 0.6772 - val_loss: 1.8924 - val_acc: 0.8192\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 4s 646us/step - loss: 1.2548 - acc: 0.8984 - val_loss: 1.4754 - val_acc: 0.8431\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 4s 667us/step - loss: 1.1348 - acc: 0.9407 - val_loss: 1.1843 - val_acc: 0.8311\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 4s 660us/step - loss: 1.0718 - acc: 0.9642 - val_loss: 0.9626 - val_acc: 0.8371\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 4s 653us/step - loss: 1.0391 - acc: 0.9747 - val_loss: 0.8238 - val_acc: 0.8479\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 4s 657us/step - loss: 1.0102 - acc: 0.9835 - val_loss: 0.7797 - val_acc: 0.8383\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 5s 713us/step - loss: 0.9879 - acc: 0.9906 - val_loss: 0.7262 - val_acc: 0.8443\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 5s 763us/step - loss: 0.9718 - acc: 0.9943 - val_loss: 0.7201 - val_acc: 0.8503\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 5s 749us/step - loss: 0.9681 - acc: 0.9928 - val_loss: 0.7022 - val_acc: 0.8503\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 5s 770us/step - loss: 0.9582 - acc: 0.9949 - val_loss: 0.7067 - val_acc: 0.8467\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 8s 1ms/step - loss: 2.1052 - acc: 0.6796 - val_loss: 1.9096 - val_acc: 0.8132\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 4s 648us/step - loss: 1.2501 - acc: 0.9022 - val_loss: 1.5289 - val_acc: 0.8383\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 4s 655us/step - loss: 1.1284 - acc: 0.9470 - val_loss: 1.2168 - val_acc: 0.8263\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 4s 632us/step - loss: 1.0828 - acc: 0.9608 - val_loss: 0.9778 - val_acc: 0.8359\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 4s 634us/step - loss: 1.0375 - acc: 0.9771 - val_loss: 0.8276 - val_acc: 0.8527\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 4s 634us/step - loss: 1.0117 - acc: 0.9828 - val_loss: 0.7630 - val_acc: 0.8455\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 4s 634us/step - loss: 0.9890 - acc: 0.9895 - val_loss: 0.7418 - val_acc: 0.8383\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 4s 643us/step - loss: 0.9759 - acc: 0.9922 - val_loss: 0.7184 - val_acc: 0.8371\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 4s 636us/step - loss: 0.9648 - acc: 0.9945 - val_loss: 0.7136 - val_acc: 0.8455\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 4s 653us/step - loss: 0.9546 - acc: 0.9961 - val_loss: 0.7038 - val_acc: 0.8551\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 7s 1ms/step - loss: 2.0501 - acc: 0.6916 - val_loss: 1.9288 - val_acc: 0.8072\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 4s 636us/step - loss: 1.2521 - acc: 0.8987 - val_loss: 1.5260 - val_acc: 0.8168\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 4s 634us/step - loss: 1.1317 - acc: 0.9437 - val_loss: 1.2181 - val_acc: 0.8192\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 4s 636us/step - loss: 1.0690 - acc: 0.9681 - val_loss: 0.9847 - val_acc: 0.8371\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 5s 697us/step - loss: 1.0326 - acc: 0.9798 - val_loss: 0.8520 - val_acc: 0.8395\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 4s 646us/step - loss: 1.0073 - acc: 0.9864 - val_loss: 0.7672 - val_acc: 0.8371\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 4s 648us/step - loss: 0.9879 - acc: 0.9910 - val_loss: 0.7236 - val_acc: 0.8443\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 4s 646us/step - loss: 0.9722 - acc: 0.9936 - val_loss: 0.7239 - val_acc: 0.8551\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 4s 643us/step - loss: 0.9608 - acc: 0.9955 - val_loss: 0.7165 - val_acc: 0.8491\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 4s 646us/step - loss: 0.9557 - acc: 0.9954 - val_loss: 0.7083 - val_acc: 0.8383\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 7s 1ms/step - loss: 2.0827 - acc: 0.6843 - val_loss: 1.8858 - val_acc: 0.8036\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 4s 636us/step - loss: 1.2469 - acc: 0.9040 - val_loss: 1.4921 - val_acc: 0.8359\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 4s 648us/step - loss: 1.1346 - acc: 0.9419 - val_loss: 1.1946 - val_acc: 0.8419\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 4s 641us/step - loss: 1.0735 - acc: 0.9651 - val_loss: 0.9447 - val_acc: 0.8467\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 4s 639us/step - loss: 1.0355 - acc: 0.9777 - val_loss: 0.8332 - val_acc: 0.8371\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 4s 643us/step - loss: 1.0085 - acc: 0.9859 - val_loss: 0.7590 - val_acc: 0.8455\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 5s 713us/step - loss: 0.9897 - acc: 0.9891 - val_loss: 0.7107 - val_acc: 0.8467\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 4s 650us/step - loss: 0.9747 - acc: 0.9924 - val_loss: 0.6997 - val_acc: 0.8431\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 4s 646us/step - loss: 0.9638 - acc: 0.9936 - val_loss: 0.6808 - val_acc: 0.8527\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 4s 643us/step - loss: 0.9507 - acc: 0.9960 - val_loss: 0.6930 - val_acc: 0.8443\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 8s 1ms/step - loss: 2.0986 - acc: 0.6808 - val_loss: 1.9134 - val_acc: 0.8036\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 4s 653us/step - loss: 1.2491 - acc: 0.9028 - val_loss: 1.5078 - val_acc: 0.8216\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 4s 662us/step - loss: 1.1348 - acc: 0.9445 - val_loss: 1.2118 - val_acc: 0.8263\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 4s 643us/step - loss: 1.0665 - acc: 0.9683 - val_loss: 0.9674 - val_acc: 0.8455\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 4s 646us/step - loss: 1.0310 - acc: 0.9784 - val_loss: 0.8231 - val_acc: 0.8371\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 4s 655us/step - loss: 1.0072 - acc: 0.9841 - val_loss: 0.7493 - val_acc: 0.8551\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 4s 643us/step - loss: 0.9861 - acc: 0.9912 - val_loss: 0.7230 - val_acc: 0.8515\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 4s 641us/step - loss: 0.9747 - acc: 0.9915 - val_loss: 0.6955 - val_acc: 0.8491\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 4s 655us/step - loss: 0.9666 - acc: 0.9928 - val_loss: 0.6937 - val_acc: 0.8407\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 4s 655us/step - loss: 0.9572 - acc: 0.9954 - val_loss: 0.7010 - val_acc: 0.8491\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 8s 1ms/step - loss: 2.0927 - acc: 0.6781 - val_loss: 1.9012 - val_acc: 0.8132\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 4s 664us/step - loss: 1.2509 - acc: 0.8991 - val_loss: 1.4899 - val_acc: 0.8419\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 5s 697us/step - loss: 1.1318 - acc: 0.9460 - val_loss: 1.2089 - val_acc: 0.8311\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 4s 641us/step - loss: 1.0744 - acc: 0.9666 - val_loss: 0.9759 - val_acc: 0.8359\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 4s 646us/step - loss: 1.0309 - acc: 0.9805 - val_loss: 0.8227 - val_acc: 0.8431\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 4s 636us/step - loss: 1.0064 - acc: 0.9843 - val_loss: 0.7466 - val_acc: 0.8431\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 4s 636us/step - loss: 0.9872 - acc: 0.9909 - val_loss: 0.7127 - val_acc: 0.8383\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 4s 655us/step - loss: 0.9729 - acc: 0.9921 - val_loss: 0.7102 - val_acc: 0.8527\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 4s 653us/step - loss: 0.9629 - acc: 0.9942 - val_loss: 0.7131 - val_acc: 0.8443\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 4s 662us/step - loss: 0.9523 - acc: 0.9951 - val_loss: 0.6957 - val_acc: 0.8455\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 8s 1ms/step - loss: 2.0751 - acc: 0.6816 - val_loss: 1.9136 - val_acc: 0.8204\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 4s 655us/step - loss: 1.2520 - acc: 0.9039 - val_loss: 1.4890 - val_acc: 0.8240\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 4s 671us/step - loss: 1.1356 - acc: 0.9470 - val_loss: 1.1845 - val_acc: 0.8323\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 5s 678us/step - loss: 1.0733 - acc: 0.9641 - val_loss: 0.9806 - val_acc: 0.8359\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 5s 716us/step - loss: 1.0353 - acc: 0.9792 - val_loss: 0.8058 - val_acc: 0.8419\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 5s 723us/step - loss: 1.0100 - acc: 0.9847 - val_loss: 0.7599 - val_acc: 0.8419\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 5s 676us/step - loss: 0.9883 - acc: 0.9901 - val_loss: 0.7366 - val_acc: 0.8299\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 4s 641us/step - loss: 0.9783 - acc: 0.9913 - val_loss: 0.7234 - val_acc: 0.8299\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 4s 639us/step - loss: 0.9633 - acc: 0.9954 - val_loss: 0.6975 - val_acc: 0.8419\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 4s 641us/step - loss: 0.9559 - acc: 0.9960 - val_loss: 0.6898 - val_acc: 0.8323\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 8s 1ms/step - loss: 2.1048 - acc: 0.6832 - val_loss: 1.9220 - val_acc: 0.8120\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 4s 657us/step - loss: 1.2569 - acc: 0.9009 - val_loss: 1.5230 - val_acc: 0.8228\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 4s 664us/step - loss: 1.1379 - acc: 0.9437 - val_loss: 1.1934 - val_acc: 0.8383\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 4s 648us/step - loss: 1.0762 - acc: 0.9644 - val_loss: 0.9602 - val_acc: 0.8407\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 4s 643us/step - loss: 1.0338 - acc: 0.9786 - val_loss: 0.8284 - val_acc: 0.8467\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 4s 643us/step - loss: 1.0123 - acc: 0.9837 - val_loss: 0.7465 - val_acc: 0.8431\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 4s 641us/step - loss: 0.9893 - acc: 0.9904 - val_loss: 0.7161 - val_acc: 0.8491\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 5s 711us/step - loss: 0.9722 - acc: 0.9942 - val_loss: 0.7129 - val_acc: 0.8455\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 4s 641us/step - loss: 0.9600 - acc: 0.9955 - val_loss: 0.6897 - val_acc: 0.8515\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 4s 641us/step - loss: 0.9560 - acc: 0.9957 - val_loss: 0.7022 - val_acc: 0.8419\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 8s 1ms/step - loss: 2.0964 - acc: 0.6743 - val_loss: 1.9257 - val_acc: 0.8048\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 5s 676us/step - loss: 1.2571 - acc: 0.8985 - val_loss: 1.5212 - val_acc: 0.8192\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 4s 648us/step - loss: 1.1340 - acc: 0.9463 - val_loss: 1.2043 - val_acc: 0.8275\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 4s 648us/step - loss: 1.0782 - acc: 0.9627 - val_loss: 0.9796 - val_acc: 0.8371\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 4s 660us/step - loss: 1.0346 - acc: 0.9768 - val_loss: 0.8382 - val_acc: 0.8347\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 4s 650us/step - loss: 1.0100 - acc: 0.9855 - val_loss: 0.8016 - val_acc: 0.8359\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 4s 650us/step - loss: 0.9866 - acc: 0.9897 - val_loss: 0.7417 - val_acc: 0.8419\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 4s 648us/step - loss: 0.9741 - acc: 0.9925 - val_loss: 0.7208 - val_acc: 0.8407\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 4s 648us/step - loss: 0.9660 - acc: 0.9942 - val_loss: 0.7295 - val_acc: 0.8467\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 4s 648us/step - loss: 0.9556 - acc: 0.9954 - val_loss: 0.7068 - val_acc: 0.8407\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 8s 1ms/step - loss: 2.1095 - acc: 0.6740 - val_loss: 1.9099 - val_acc: 0.8060\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 4s 667us/step - loss: 1.2469 - acc: 0.9040 - val_loss: 1.5082 - val_acc: 0.8383\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 4s 653us/step - loss: 1.1359 - acc: 0.9400 - val_loss: 1.1929 - val_acc: 0.8347\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 4s 650us/step - loss: 1.0696 - acc: 0.9660 - val_loss: 0.9814 - val_acc: 0.8275\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 5s 688us/step - loss: 1.0381 - acc: 0.9759 - val_loss: 0.8470 - val_acc: 0.8335\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 5s 683us/step - loss: 1.0075 - acc: 0.9865 - val_loss: 0.7849 - val_acc: 0.8371\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 5s 695us/step - loss: 0.9914 - acc: 0.9895 - val_loss: 0.7475 - val_acc: 0.8228\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 5s 692us/step - loss: 0.9773 - acc: 0.9919 - val_loss: 0.7071 - val_acc: 0.8359\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 5s 676us/step - loss: 0.9627 - acc: 0.9936 - val_loss: 0.7147 - val_acc: 0.8323\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 5s 692us/step - loss: 0.9590 - acc: 0.9934 - val_loss: 0.6958 - val_acc: 0.8407\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 8s 1ms/step - loss: 2.1130 - acc: 0.6711 - val_loss: 1.9007 - val_acc: 0.7940\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 5s 681us/step - loss: 1.2460 - acc: 0.9069 - val_loss: 1.5211 - val_acc: 0.8287\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 5s 678us/step - loss: 1.1383 - acc: 0.9389 - val_loss: 1.2050 - val_acc: 0.8383\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 5s 683us/step - loss: 1.0730 - acc: 0.9663 - val_loss: 0.9712 - val_acc: 0.8347\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 5s 690us/step - loss: 1.0374 - acc: 0.9778 - val_loss: 0.8264 - val_acc: 0.8359\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 5s 690us/step - loss: 1.0051 - acc: 0.9856 - val_loss: 0.7595 - val_acc: 0.8419\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 5s 697us/step - loss: 0.9929 - acc: 0.9888 - val_loss: 0.7139 - val_acc: 0.8311\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 5s 683us/step - loss: 0.9799 - acc: 0.9901 - val_loss: 0.6998 - val_acc: 0.8359\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 5s 683us/step - loss: 0.9660 - acc: 0.9940 - val_loss: 0.6826 - val_acc: 0.8407\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 5s 688us/step - loss: 0.9559 - acc: 0.9948 - val_loss: 0.6786 - val_acc: 0.8431\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "Xcept = [Sequential() for i in range (20)]\n",
    "\n",
    "\n",
    "def buildModel (model):\n",
    "    model.add(GlobalAveragePooling2D(input_shape=train_xception.shape[1:]))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(133, activation='linear'))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    indxes = np.random.randint(0,high= train_xception.shape[0], size = train_xception.shape[0], dtype = 'int32')\n",
    "    train = np.array([train_xception[ind] for ind in indexes])\n",
    "    targ = np.array([train_targets[ind]for ind in indexes])\n",
    "                     \n",
    "    model.fit(train, targ, \n",
    "          validation_data=(valid_xception, valid_targets),\n",
    "          epochs=10, batch_size=100, verbose=1)\n",
    "for model in Xcept:\n",
    "    buildModel(model)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "xception_predictions =[]\n",
    "for feature in test_xception:\n",
    "    vector = np.array([model.predict(np.expand_dims(feature, axis=0)) for model in Xcept])\n",
    "    xception_prediction = vector.sum(axis=0)\n",
    "    xception_predictions.append(np.argmax(xception_prediction))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 86.6029%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = 100*np.sum(np.array(xception_predictions)==np.argmax(test_targets, axis=1))/len(xception_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "xception_predictions1 = []\n",
    "for feature in test_xception:\n",
    "    vector = np.array([model.predict(np.expand_dims(feature, axis=0)) for model in Xcept])\n",
    "    vector.shape = (-1, vector.shape[2])\n",
    "    vector1 = np.array([np.argmax(vector[i]) for i in range(len(vector))])\n",
    "    xception_prediction = np.bincount(vector1)\n",
    "    xception_predictions1.append(np.argmax(xception_prediction))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 86.6029%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = 100*np.sum(np.array(xception_predictions1)==np.argmax(test_targets, axis=1))/len(xception_predictions1)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(836,) (836,)\n"
     ]
    }
   ],
   "source": [
    "print (np.array(xception_predictions1).shape, np.array(xception_predictions).shape)\n",
    "sameness = 100*np.sum(np.array(xception_predictions1)== np.array(xception_predictions))/len(xception_predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sameness: 98.8038%\n",
      "23 95 94\n",
      "34 50 105\n",
      "40 80 120\n",
      "44 65 34\n",
      "52 25 73\n",
      "72 81 100\n",
      "75 76 52\n",
      "93 55 101\n",
      "98 8 34\n",
      "110 81 98\n",
      "120 120 69\n",
      "123 0 98\n",
      "128 43 40\n",
      "134 17 58\n",
      "146 83 66\n",
      "148 123 87\n",
      "158 111 85\n",
      "159 112 16\n",
      "164 65 34\n",
      "168 115 47\n",
      "175 36 62\n",
      "181 99 100\n",
      "190 15 6\n",
      "192 50 20\n",
      "202 17 69\n",
      "222 83 44\n",
      "255 113 88\n",
      "258 63 45\n",
      "271 20 67\n",
      "287 111 66\n",
      "306 52 60\n",
      "334 48 23\n",
      "341 78 94\n",
      "346 127 42\n",
      "348 132 126\n",
      "351 25 73\n",
      "368 17 58\n",
      "383 132 106\n",
      "390 127 42\n",
      "395 75 85\n",
      "414 47 101\n",
      "419 62 84\n",
      "423 124 87\n",
      "442 74 41\n",
      "443 54 34\n",
      "445 34 85\n",
      "447 52 45\n",
      "452 95 46\n",
      "475 47 109\n",
      "485 66 75\n",
      "493 98 81\n",
      "496 88 72\n",
      "500 35 16\n",
      "501 44 79\n",
      "504 20 21\n",
      "516 59 39\n",
      "532 84 129\n",
      "534 30 38\n",
      "537 127 38\n",
      "552 21 20\n",
      "566 110 106\n",
      "571 84 111\n",
      "573 105 88\n",
      "575 132 48\n",
      "600 55 1\n",
      "604 71 120\n",
      "628 40 102\n",
      "659 95 71\n",
      "662 60 65\n",
      "692 132 126\n",
      "698 0 37\n",
      "728 9 6\n",
      "742 3 70\n",
      "745 36 15\n",
      "756 111 11\n",
      "780 9 120\n",
      "783 24 26\n",
      "815 94 75\n"
     ]
    }
   ],
   "source": [
    "print('Sameness: %.4f%%' % sameness)\n",
    "\n",
    "for i, target in enumerate(xception_predictions):\n",
    "    if target != xception_predictions1[i]:\n",
    "        print (i, target, xception_predictions1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = np.array([[1,2], [2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.sum(axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.93403024e-04,   2.91732344e-04,   7.08184845e-04,\n",
       "          5.53736056e-04,   8.82767723e-04,   5.65732887e-04,\n",
       "          5.29449550e-04,   2.93705409e-04,   1.10657529e-04,\n",
       "          3.76875891e-04,   5.58995525e-04,   4.05269791e-04,\n",
       "          3.29000788e-04,   3.35938646e-04,   5.50363737e-04,\n",
       "          1.40384375e-03,   6.56558026e-04,   5.88758790e-04,\n",
       "          7.13188783e-04,   5.64187067e-04,   1.72838022e-03,\n",
       "          7.18943193e-04,   5.27972705e-04,   8.41362576e-04,\n",
       "          2.95340404e-04,   9.37157383e-05,   4.06838109e-04,\n",
       "          3.13604425e-04,   6.50361064e-04,   5.83247398e-04,\n",
       "          3.50395625e-04,   5.58093190e-04,   2.92699609e-04,\n",
       "          2.71333818e-04,   6.56212505e-04,   1.12004590e-03,\n",
       "          4.38363699e-04,   2.13358202e-04,   1.98076916e-04,\n",
       "          1.04406138e-03,   3.25224741e-04,   3.92265065e-04,\n",
       "          6.42737548e-04,   4.45378653e-04,   3.89926689e-04,\n",
       "          5.67363808e-04,   6.54491305e-04,   4.82334144e-04,\n",
       "          7.85042474e-04,   3.35395074e-04,   3.71008384e-04,\n",
       "          1.47924409e-04,   9.19120212e-04,   8.01130547e-04,\n",
       "          7.75552762e-04,   9.74590366e-04,   9.27732468e-01,\n",
       "          4.47906437e-04,   3.60667589e-04,   6.99584896e-04,\n",
       "          1.47586776e-04,   1.29271427e-03,   7.05670565e-04,\n",
       "          4.18104522e-04,   5.99200430e-04,   5.62529429e-04,\n",
       "          8.55659600e-04,   2.28538047e-04,   3.03216220e-04,\n",
       "          7.52306136e-04,   2.38883324e-04,   8.57213512e-04,\n",
       "          3.37319565e-04,   1.35259645e-04,   7.68180005e-04,\n",
       "          4.87899728e-04,   4.53987333e-04,   6.19905186e-04,\n",
       "          3.01303371e-04,   8.66690942e-04,   2.04383265e-04,\n",
       "          8.33072118e-05,   4.14950133e-04,   1.80958217e-04,\n",
       "          2.43405826e-04,   5.99890831e-04,   9.67264350e-04,\n",
       "          3.09666153e-04,   7.23414356e-04,   6.69575820e-04,\n",
       "          1.32586889e-03,   2.55542691e-04,   6.27959031e-04,\n",
       "          7.61983916e-04,   1.72209926e-03,   5.03841904e-04,\n",
       "          4.77155350e-04,   1.64896541e-04,   2.76374718e-04,\n",
       "          2.10432743e-04,   9.32698895e-04,   7.16712151e-04,\n",
       "          2.93012126e-04,   2.64752540e-04,   8.66027491e-04,\n",
       "          2.39079309e-04,   5.54089493e-04,   1.02036598e-03,\n",
       "          4.78811620e-04,   7.58470851e-04,   1.72137597e-03,\n",
       "          4.11232002e-04,   2.76844832e-04,   3.25021974e-04,\n",
       "          3.35500488e-04,   1.93973086e-04,   3.42486048e-04,\n",
       "          6.42470957e-04,   6.92819653e-04,   5.82784007e-04,\n",
       "          2.19126145e-04,   5.83291345e-04,   7.22685130e-04,\n",
       "          2.81892659e-04,   4.89680853e-04,   3.61064565e-04,\n",
       "          1.15412206e-03,   2.86924449e-04,   1.63059696e-04,\n",
       "          1.97554909e-04,   5.75395068e-04,   5.03788062e-04,\n",
       "          1.92822001e-04]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xception_model.predict(np.expand_dims(train_xception[1],axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6680, 7, 7, 2048)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_xception.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.random.randint(0,high= train_xception.shape[0], size = train_xception.shape[0], dtype = 'int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2599, 2323, 3899, ..., 3744,  155, 6589])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array([train_xception[ind] for ind in indexes])\n",
    "targ = np.array([train_targets[ind]for ind in indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6680, 7, 7, 2048)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6680, 133)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(train_targets[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "nb35",
   "language": "python",
   "name": "nb35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
